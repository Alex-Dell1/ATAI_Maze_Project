\section{Background}\label{sec:back}

\subsection{HYPER}
HYPER (Hypotheses refiner) is an ILP program, which has been developed by Prof. Dr. Ivan Bratko in 1999.\\
The following inputs must be given to the tool:
\begin{itemize}
    \item \textbf{Background Knowledge \((BK)\).} A set of logic formulas from which the positive examples can be derived.
    \item \textbf{Replacement of structured terms \((T)\).} Knowledge of how to refine structured terms like lists or coordinates.
    \item \textbf{Start clause \((S)\).} The starting hypothesis, which will be refined. 
    \item \textbf{Positive Examples \((E^+)\).}
    \item \textbf{Negative Examples \((E^-)\).}
\end{itemize}
The learning procedure is as follows:
\begin{itemize}
    \item \textbf{Choose a start hypothesis.} It is important to choose it general enough to be complete (i.e. cover all positive examples).
    \item \textbf{Continuously refine the hypothesis by:} 
	\begin{enumerate}
	\item Matching two variables of the same type or
	\item Adding a goal from the background knowledge or
	\item Refining a variable to background terms
	\end{enumerate}
\end{itemize}
Details on HYPER can be found in "Prolog Programming for Artificial Intelligence" by Ivan Bratko (4th edition, chapter 21).
\subsection{Metagol}
Metagol is a system used for ILP, which relies on meta-interpretative learning.\\
Using Metagol, four key components need to be defined:
\begin{itemize}
    \item \textbf{Metarules \((M)\).} Metarules are used to define the \emph{language bias} of the task. A large number metarules allows for a less strict language bias, hence a larger search space in which to find a solution. 
    \item \textbf{Background Knowledge \((BK)\).} The knowledge the system is initially assumed to have about the task to be carried out. It is a set of Prolog rules that the system can use either directly or indirectly in order to induce the hypothesis.
    \item \textbf{Positive Examples \((E^+)\).}
    \item \textbf{Negative Examples \((E^-)\).}
\end{itemize}
With these four components defined, Metagol will try to find a solution running the following algorithm:
\begin{enumerate}
    \item Select a positive example to be proven.
    \item Try to prove the example using the existing \(BK\) or previously induced clauses.
    \item (If step 2 did not work) Unify the example with the head of a metarule and repeat steps 1,2 and 3 for each atom in the body of the obtained rule.
    \item Once the hypothesis is proven to be complete (all the positive examples have been proven and covered), test its consistency. If any negative example is covered, backtrack to a choice made in step 3 which, supposedly, led to this situation.
\end{enumerate}
In this brief illustration of Metagol, the process of \emph{predicate invention} is not covered due to a lack of time to further study it. Our findings about this process mainly derive
from experimental experience and have no theoretical backup. Nonetheless, we will still point out the influence it had on our results.


\subsection{ILASP}

ILASP enables learning programs containing normal rules, choice rules and both hard and weak constraints, these are the rules that compose ASP econdings and here this tool will be used for the maze problem. Weak constraints won't be covered, the goal here is to try to learn some rules that will form an encoding of that problem.

Similarly to what presented above for Metagol, providing to ILASP Background knowledge, language Bias, Positive and negative examples, it's possible to learn rules inductivly.


