\subsection{HYPER}\label{sec:hyper}
The scripts discussed in this section of the report can be found in the \texttt{HYPER} folder.\\

\subsubsection{\texttt{Defining the maze}}

The necessary background knowledge to find a way through a labyrinth initially includes the dimensions of the labyrinth, the positions of the obstacles and the start and finish positions. For HYPER, the 5x5 maze was defined as follows:
\begin{lstlisting}[label={lst:maze}, language=Prolog, caption=Definition of the maze, belowcaptionskip=1cm]
size(L) :- numlist(1,5,L).
obstacle((1,2)).
obstacle((2,2)).
obstacle((3,2)).
obstacle((4,4)).
obstacle((3,4)).
obstacle((2,4)).
obstacle((1,4)).
obstacle((1,5)).
obstacle((5,1)).
start((1,1)).
goal((2,5)).
\end{lstlisting}
\subsubsection{\texttt{Learning adjacent cells}}
The first task is to learn, which cells are adjacent to each other. Only between adjacent cells moves are possible (not considering the obstacles yet).\\
The background knowledge consists of the "next" predicate, which defines which integer numbers are within the grid size and are predecessors respectively succesors of each other.
As the "adjacent" predicate should refer to cells, which are represented by coordinates X and Y, also the knowledge of how to refine a cell to integer values needs to be included.\\
Listing \ref{lst:adjacent} shows the corresponding code snippet.
\begin{lstlisting}[label={lst:adjacent}, language=Prolog, caption=Learning the predicate "adjacent", belowcaptionskip=1cm]
Background literal:
next( X, Y):- 
	size(L),
	member( X, L),
	member( Y, L),
	(Y is X+1; Y is X-1).
backliteral( next( X, Y), [X:integer], [Y:integer]).

Refinement of terms:
term( cell, (A, B), [A:integer, B:integer]).

Background predicate:
prolog_predicate(next(_,_)).

Start clause:
start_clause( [adjacent( X, Y)] / [ X:cell, Y:cell] ). 
\end{lstlisting}
The following parameters are used to run the "adjacent" task:
\begin{lstlisting}[label={lst:adjacent_params}, language=Prolog, caption=Parameters for learning "adjacent", belowcaptionskip=1cm]
max_proof_length( 2).   
max_clauses( 2).        
max_clause_length( 3).  
\end{lstlisting}
To solve the task correctly, HYPER needs four positive examples, one for a move in each direction, and three negative examples, preventing moves to diagonal cells or far away cells.
The concrete examples can be found in the source code. HYPER generates 29,870 hypotheses of which 4,687 are refined and 11,482 are left to be refined, leading to the following "adjacent" predicate:
\begin{lstlisting}[label={lst:adjacent_res}, language=Prolog, caption=Predicate "adjacent", belowcaptionskip=1cm]
adjacent((A,B),(C,B)):-
  next(A,C).
adjacent((A,B),(A,C)):-
  next(B,C).
\end{lstlisting}
Adding four more positive examples (again one for each direction, so in sum having two for each direction) leads to exactly the same result with the same performance.\\
Providing more negative examples of the same type (diagonal, far jumps) still leads to the correct result, but the performance decreases:
Adding another diagonal move, 34,701 hypotheses have to be generated (5,492 refined, 13,588 left to be refined). Additionally adding another positive example does not impact the result. 
Adding another far jump example further increases the number of hypotheses generated to 35,577 (5,720 refined, 14,068 left to be refined).\\
Interestingly, for a correct result it is not necessary to give a negative example of the form "two cells are not adjacent if they are the same cell". On the contrary, adding such a negative example only increases the running time and the number of hypotheses generated to 34,950:
\begin{lstlisting}[label={lst:adjacent_neg}, language=Prolog, caption=Negative example, belowcaptionskip=1cm]
nex( adjacent( (1,2), (1,2))).
\end{lstlisting}
\subsubsection{\texttt{Learning to walk}}
Having learned the predicate "adjacent", the next step is to learn to move from cell to cell, not hitting any obstacle.
The obstacles and the "adjacent" predicate constitute the background knowledge for the "move" task:
\begin{lstlisting}[label={lst:move}, language=Prolog, caption=Learning the predicate "move", belowcaptionskip=1cm]
Background literals:
backliteral( obstacle(X), [X:cell], []).
backliteral( \+ (G), [X:cell], []) :-
	G = obstacle(X).
backliteral( adjacent(X,Y), [X:cell,Y:cell], []). 

Refinement of terms:
term( fail, fail, fail).

Background predicates:
prolog_predicate( obstacle(_)).
prolog_predicate( adjacent(_,_)).
prolog_predicate( \+(_)).       

Start clause:
start_clause( [move(X,Y)] / [ X:cell, Y:cell] ).
\end{lstlisting}
The parameters used to learn the predicate "move" are the same as for the predicate "adjacent" in listing \ref{lst:adjacent_params}.\\
To solve the task HYPER only needs to generate 13 hypotheses of which 2 are refined and 5 are left to be refined:
\begin{lstlisting}[label={lst:move_res}, language=Prolog, caption= Predicate "move", belowcaptionskip=1cm]
move(A,B):-
  adjacent(B,A),
  \+obstacle(B).
\end{lstlisting}
To learn the predicate only one positive and one negative example are necessary:
\begin{lstlisting}[label={lst:move_examples}, language=Prolog, caption= Examples to learn "move", belowcaptionskip=1cm]
ex( move( (2,1), (3,1))).
nex( move( (2,1), (2,2))).
\end{lstlisting}
Adding five more positive examples neither changes the result nor the performance.
In this case the same holds for adding more moves from free cells to obstacle cells as negative examples.
Considering that it should not be possible not to move at all also has no impact on the result:
\begin{lstlisting}[label={lst:move_neg}, language=Prolog, caption= Additional negative example for "move", belowcaptionskip=1cm]
nex( move( (1,2), (1,2))).
\end{lstlisting}
The learning algorithm is independent of the defined grid and the grid size:
{\rowcolors{2}{gray!50!}{}
\begin{center}
    \begin{table}[h]
    \centering
    \begin{tabular}{ |c|c|c| } 
        \hline
        \texttt{Grid size} & \texttt{\# Hypotheses generated} & \texttt{\# Hypotheses refined} \\ \hline
        5x5 & 13 & 2 \\ 
        7x7 & 13 & 2 \\ 
        9x9 & 13 & 2 \\ 
        \hline
    \end{tabular}
    \caption{\label{tab:move_grid}\texttt{Dependence on grid size} }
    \end{table}
\end{center}
}

\subsubsection{\texttt{Learning to travel}}
In order to cover longer distances in the labyrinth, several moves must be made one after the other.
To learn to travel is a prerequisite for finding paths in the maze. All cells visited on the path should be stored in a list, which leads to a recursive task.
Therefore also the predicate "reach", which has to be learned, has to be put as backliteral.
Additionally, it has to be provided in the term clause, how lists can be refined.
\begin{lstlisting}[label={lst:travel}, language=Prolog, caption=Learning the predicate "move", belowcaptionskip=1cm]
Background literals:
backliteral( move(X,Y), [X:cell], [Y:cell]).
backliteral( reach(X,Y,L), [X:cell,L:list], [Y:cell]).
 
Refinement of terms
term( list1, [X|L], [ X:cell, L:list]).
term( list1, [X], [X:cell]).
	
Background predicates
prolog_predicate( move(_,_)).

Start clause:
start_clause( [ reach(X,Y,L)] / [X:cell,Y:cell,L:list1] ).    
\end{lstlisting}
It is crucial to update the parameters:
\begin{lstlisting}[label={lst:reach_params}, language=Prolog, caption=Parameters for learning "reach", belowcaptionskip=1cm]
max_proof_length( 5).   
max_clauses( 2).        
max_clause_length( 3).  
\end{lstlisting}
To solve the task only one positive and four negative examples are sufficient:
\begin{lstlisting}[label={lst:reach_examples}, language=Prolog, caption= Examples for "reach", belowcaptionskip=1cm]
ex( reach( (3,1), (4,2), [(3,1), (4,1), (4,2)])).
nex( reach( (1,1), (4,1), [(1,1), (2,1), (3,1), (4,3)])).
nex( reach( (2,3), (2,3), [(1,1)])).
nex( reach( (2,1), (3,3), [(2,1), (2,2), (2,3),(2,2)])).
nex( reach( (3,1), (4,2), [(3,1), (4,1), (3,1)])).
\end{lstlisting}
HYPER generates 264 hypotheses to solve the task of which 60 are refined and 47 are left to be refined.\\
The following listing \ref{lst:reach_res} shows the learned predicate "reach":
\begin{lstlisting}[label={lst:reach_res}, language=Prolog, caption= Predicate "reach", belowcaptionskip=1cm]
reach(A,A,[A]).
reach(A,D,[A|B]):-
  move(A,C),
  reach(C,D,B).
\end{lstlisting}
Adding another positive example, the number of hypotheses refined can be reduced to 46:
\begin{lstlisting}[label={lst:reach_ex1}, language=Prolog, caption= Additional positive example for "reach", belowcaptionskip=1cm]
ex( reach( (1,1), (2,1), [(1,1), (2,1)])).
\end{lstlisting}
Adding more positive or negative examples does not seem to change the result or the performance. The source code contains all examples that were added on a trial basis.
\subsubsection{\texttt{Combined Learning: walk \& travel}}
Giving sufficient background knowledge and more start clauses, it is also possible to learn more predicates at once, for example the predicates "move" and "reach":
\begin{lstlisting}[label={lst:comb}, language=Prolog, caption=Combined learning of "move" \& "reach", belowcaptionskip=1cm]
Background literals:
backliteral( obstacle(X), [X:cell], []).
backliteral( \+ (G), [X:cell], []) :- 	G = obstacle(X).
backliteral( adjacent(X,Y), [X:cell,Y:cell], []).
backliteral( move(X,Y), [X:cell], [Y:cell]).
backliteral( reach(X,Y,L), [X:cell,L:list], [Y:cell]).

Refinement of terms
term( list1, [X|L], [ X:cell, L:list]).
term( list1, [X], [X:cell]).
	
		
Background predicates
prolog_predicate( obstacle(_)).
prolog_predicate( adjacent(_,_)).
prolog_predicate( \+(_)).       

Start clauses:
start_clause( [move(X,Y)] / [ X:cell, Y:cell] ).
start_clause( [ reach(X,Y,L)] / [X:cell,Y:cell,L:list1] ).  
\end{lstlisting}
To find a solution, it is sufficient to add all positive and negative examples of the individual tasks together. The number of hypotheses generated in this case is 3,821, the number of hypotheses refined is 483.
\begin{lstlisting}[label={lst:comb_ex}, language=Prolog, caption=Examples for combined learning of "move" \& "reach", belowcaptionskip=1cm]
ex( move( (2,1), (3,1))).
nex( move( (2,1), (2,2))). 
ex( reach( (1,1), (2,1), [(1,1), (2,1)])). 
ex( reach( (3,1), (4,2), [(3,1), (4,1), (4,2)])).
nex( reach( (1,1), (4,1), [(1,1), (2,1), (3,1), (4,3)])).
nex( reach( (2,3), (2,3), [(1,1)])).
nex( reach( (2,1), (3,3), [(2,1), (2,2), (2,3),(2,2)])).
nex( reach( (3,1), (4,2), [(3,1), (4,1), (3,1)])).
\end{lstlisting}
Giving another negative eample the number of hypotheses generated can be improved to 3,034:
\begin{lstlisting}[label={lst:comb_neg}, language=Prolog, caption=Additional example for combined learning of "move" \& "reach", belowcaptionskip=1cm]
nex( reach( (4,2), (4,2), [])).
\end{lstlisting}
Adding the following two negative examples, heavily increases the number of hypotheses generated to 31,633, of which 2,154 are refined. Increasing the parameters does not resolve this issue.
\begin{lstlisting}[label={lst:comb_neg2}, language=Prolog, caption=Additional example for combined learning of "move" \& "reach", belowcaptionskip=1cm]
nex( reach( (4,2), (4,2), [])).
nex( reach( (2,1), (3,3), [(2,2), (2,3), (3,3)])).
\end{lstlisting}
However, only adding the last negative example, the number of hypotheses generated can be significantly reduced to 1,899, of which 163 are refined:
\begin{lstlisting}[label={lst:comb_neg3}, language=Prolog, caption=Additional example for combined learning of "move" \& "reach", belowcaptionskip=1cm]
nex( reach( (2,1), (3,3), [(2,2), (2,3), (3,3)])).
\end{lstlisting}
In conclusion, it can be said that the tool's performance strongly depends on the chosen examples.\\
Another conclusion is that it is easier to learn several tasks one by one than at the same time. 
The following table summarizes this finding (time measurements are mean values from ten runs):
{\rowcolors{2}{gray!50!}{}
\begin{center}
    \begin{table}[h]
    \centering
    \begin{tabular}{ |c|c|c|c|} 
        \hline
        \texttt{Task} & \texttt{Time} & \texttt{\# Hypotheses generated} & \texttt{\# Hypotheses refined} \\ \hline
        adjacent & 175.884 s & 29,870 & 4,687 \\ 
        move & 0.063 s & 13 & 2\\
        reach & 1.577 s & 265 & 46\\
        move \& reach & 7.077 s & 1,899 & 163\\
        \hline
    \end{tabular}
    \caption{\label{tab:hyper_res}\texttt{Performance of HYPER}}
    \end{table}
\end{center}
}



